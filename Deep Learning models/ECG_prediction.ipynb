{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ECG-prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9jmu_6EaXPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYvgQLZoaLsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c97088a3-a3e7-4c94-a0b7-d7186154ef78"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, utils, callbacks, metrics\n",
        "\n",
        "import cv2\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKy_HXz7aLsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 吃路徑的方式開檔案\n",
        "# FOLDER_PATH = '/content/drive/My Drive/class/勞動部/week12/ECG'\n",
        "# df = pd.read_csv(os.path.join(FOLDER_PATH, 'mitbih.csv'), header=None)\n",
        "# num_classes=5\n",
        "\n",
        "# 把檔案放入 google dirve 內直接開\n",
        "df = pd.read_csv(\"mitbih.csv\")\n",
        "num_classes = 5"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2vY5R1-aLsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "3023285e-42c3-4c74-e690-0882f6e7beb4"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.9248658418655396</th>\n",
              "      <th>0.9928443431854248</th>\n",
              "      <th>0.5366726517677307</th>\n",
              "      <th>0.1824686974287033</th>\n",
              "      <th>0.09481216222047804</th>\n",
              "      <th>0.051878355443477624</th>\n",
              "      <th>0.03577817603945732</th>\n",
              "      <th>0.06082289665937424</th>\n",
              "      <th>0.09302325546741486</th>\n",
              "      <th>0.18783542513847354</th>\n",
              "      <th>0.3398926556110382</th>\n",
              "      <th>0.4275491833686829</th>\n",
              "      <th>0.4722719192504882</th>\n",
              "      <th>0.4740608334541321</th>\n",
              "      <th>0.4132379293441773</th>\n",
              "      <th>0.4704830050468445</th>\n",
              "      <th>0.4758497178554535</th>\n",
              "      <th>0.4686940908432007</th>\n",
              "      <th>0.44722718000411993</th>\n",
              "      <th>0.44722718000411993.1</th>\n",
              "      <th>0.44722718000411993.2</th>\n",
              "      <th>0.4311270117759705</th>\n",
              "      <th>0.39892664551734924</th>\n",
              "      <th>0.39713773131370544</th>\n",
              "      <th>0.41502684354782093</th>\n",
              "      <th>0.38998210430145264</th>\n",
              "      <th>0.3506261110305786</th>\n",
              "      <th>0.3649373948574066</th>\n",
              "      <th>0.381037563085556</th>\n",
              "      <th>0.381037563085556.1</th>\n",
              "      <th>0.3631484806537628</th>\n",
              "      <th>0.3881931900978087</th>\n",
              "      <th>0.40787118673324574</th>\n",
              "      <th>0.4382826387882233</th>\n",
              "      <th>0.4543828368186951</th>\n",
              "      <th>0.4794275462627411</th>\n",
              "      <th>0.4740608334541321.1</th>\n",
              "      <th>0.499105542898178</th>\n",
              "      <th>0.5205724239349364</th>\n",
              "      <th>0.5134168267250061</th>\n",
              "      <th>...</th>\n",
              "      <th>0.0.39</th>\n",
              "      <th>0.0.40</th>\n",
              "      <th>0.0.41</th>\n",
              "      <th>0.0.42</th>\n",
              "      <th>0.0.43</th>\n",
              "      <th>0.0.44</th>\n",
              "      <th>0.0.45</th>\n",
              "      <th>0.0.46</th>\n",
              "      <th>0.0.47</th>\n",
              "      <th>0.0.48</th>\n",
              "      <th>0.0.49</th>\n",
              "      <th>0.0.50</th>\n",
              "      <th>0.0.51</th>\n",
              "      <th>0.0.52</th>\n",
              "      <th>0.0.53</th>\n",
              "      <th>0.0.54</th>\n",
              "      <th>0.0.55</th>\n",
              "      <th>0.0.56</th>\n",
              "      <th>0.0.57</th>\n",
              "      <th>0.0.58</th>\n",
              "      <th>0.0.59</th>\n",
              "      <th>0.0.60</th>\n",
              "      <th>0.0.61</th>\n",
              "      <th>0.0.62</th>\n",
              "      <th>0.0.63</th>\n",
              "      <th>0.0.64</th>\n",
              "      <th>0.0.65</th>\n",
              "      <th>0.0.66</th>\n",
              "      <th>0.0.67</th>\n",
              "      <th>0.0.68</th>\n",
              "      <th>0.0.69</th>\n",
              "      <th>0.0.70</th>\n",
              "      <th>0.0.71</th>\n",
              "      <th>0.0.72</th>\n",
              "      <th>0.0.73</th>\n",
              "      <th>0.0.74</th>\n",
              "      <th>0.0.75</th>\n",
              "      <th>0.0.76</th>\n",
              "      <th>0.0.77</th>\n",
              "      <th>0.0.78</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.981324</td>\n",
              "      <td>0.964346</td>\n",
              "      <td>0.414261</td>\n",
              "      <td>0.005093</td>\n",
              "      <td>0.139219</td>\n",
              "      <td>0.293718</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.288625</td>\n",
              "      <td>0.280136</td>\n",
              "      <td>0.276740</td>\n",
              "      <td>0.283531</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>0.283531</td>\n",
              "      <td>0.280136</td>\n",
              "      <td>0.281834</td>\n",
              "      <td>0.292020</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>0.285229</td>\n",
              "      <td>0.292020</td>\n",
              "      <td>0.298812</td>\n",
              "      <td>0.298812</td>\n",
              "      <td>0.293718</td>\n",
              "      <td>0.292020</td>\n",
              "      <td>0.305603</td>\n",
              "      <td>0.300509</td>\n",
              "      <td>0.295416</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.303905</td>\n",
              "      <td>0.308998</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.317487</td>\n",
              "      <td>0.319185</td>\n",
              "      <td>0.331070</td>\n",
              "      <td>0.336163</td>\n",
              "      <td>0.336163</td>\n",
              "      <td>0.339559</td>\n",
              "      <td>0.346350</td>\n",
              "      <td>...</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.295416</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.985486</td>\n",
              "      <td>0.804064</td>\n",
              "      <td>0.438316</td>\n",
              "      <td>0.158200</td>\n",
              "      <td>0.014514</td>\n",
              "      <td>0.074020</td>\n",
              "      <td>0.121916</td>\n",
              "      <td>0.088534</td>\n",
              "      <td>0.078374</td>\n",
              "      <td>0.092888</td>\n",
              "      <td>0.082729</td>\n",
              "      <td>0.068215</td>\n",
              "      <td>0.071118</td>\n",
              "      <td>0.075472</td>\n",
              "      <td>0.082729</td>\n",
              "      <td>0.088534</td>\n",
              "      <td>0.088534</td>\n",
              "      <td>0.100145</td>\n",
              "      <td>0.121916</td>\n",
              "      <td>0.133527</td>\n",
              "      <td>0.169811</td>\n",
              "      <td>0.197388</td>\n",
              "      <td>0.220610</td>\n",
              "      <td>0.252540</td>\n",
              "      <td>0.283019</td>\n",
              "      <td>0.325109</td>\n",
              "      <td>0.346880</td>\n",
              "      <td>0.361393</td>\n",
              "      <td>0.383164</td>\n",
              "      <td>0.397678</td>\n",
              "      <td>0.403483</td>\n",
              "      <td>0.396226</td>\n",
              "      <td>0.375907</td>\n",
              "      <td>0.370102</td>\n",
              "      <td>0.349782</td>\n",
              "      <td>0.330914</td>\n",
              "      <td>0.320755</td>\n",
              "      <td>0.297533</td>\n",
              "      <td>0.283019</td>\n",
              "      <td>0.264151</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.827320</td>\n",
              "      <td>0.755155</td>\n",
              "      <td>0.520619</td>\n",
              "      <td>0.271907</td>\n",
              "      <td>0.123711</td>\n",
              "      <td>0.059278</td>\n",
              "      <td>0.030928</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039948</td>\n",
              "      <td>0.144330</td>\n",
              "      <td>0.244845</td>\n",
              "      <td>0.291237</td>\n",
              "      <td>0.311856</td>\n",
              "      <td>0.322165</td>\n",
              "      <td>0.362113</td>\n",
              "      <td>0.407216</td>\n",
              "      <td>0.442010</td>\n",
              "      <td>0.475515</td>\n",
              "      <td>0.501289</td>\n",
              "      <td>0.518041</td>\n",
              "      <td>0.530928</td>\n",
              "      <td>0.559278</td>\n",
              "      <td>0.590206</td>\n",
              "      <td>0.606959</td>\n",
              "      <td>0.644330</td>\n",
              "      <td>0.657216</td>\n",
              "      <td>0.690722</td>\n",
              "      <td>0.713918</td>\n",
              "      <td>0.747423</td>\n",
              "      <td>0.764175</td>\n",
              "      <td>0.786082</td>\n",
              "      <td>0.789948</td>\n",
              "      <td>0.798969</td>\n",
              "      <td>0.797680</td>\n",
              "      <td>0.788660</td>\n",
              "      <td>0.771907</td>\n",
              "      <td>0.759021</td>\n",
              "      <td>0.722938</td>\n",
              "      <td>0.695876</td>\n",
              "      <td>...</td>\n",
              "      <td>0.862113</td>\n",
              "      <td>0.608247</td>\n",
              "      <td>0.568299</td>\n",
              "      <td>0.565722</td>\n",
              "      <td>0.534794</td>\n",
              "      <td>0.502577</td>\n",
              "      <td>0.48067</td>\n",
              "      <td>0.45232</td>\n",
              "      <td>0.426546</td>\n",
              "      <td>0.417526</td>\n",
              "      <td>0.399485</td>\n",
              "      <td>0.393041</td>\n",
              "      <td>0.390464</td>\n",
              "      <td>0.385309</td>\n",
              "      <td>0.382732</td>\n",
              "      <td>0.386598</td>\n",
              "      <td>0.382732</td>\n",
              "      <td>0.378866</td>\n",
              "      <td>0.376289</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.965517</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.206897</td>\n",
              "      <td>0.187739</td>\n",
              "      <td>0.118774</td>\n",
              "      <td>0.009579</td>\n",
              "      <td>0.005747</td>\n",
              "      <td>0.095785</td>\n",
              "      <td>0.203065</td>\n",
              "      <td>0.258621</td>\n",
              "      <td>0.268199</td>\n",
              "      <td>0.273946</td>\n",
              "      <td>0.289272</td>\n",
              "      <td>0.293103</td>\n",
              "      <td>0.293103</td>\n",
              "      <td>0.302682</td>\n",
              "      <td>0.312261</td>\n",
              "      <td>0.312261</td>\n",
              "      <td>0.308429</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>0.325671</td>\n",
              "      <td>0.329502</td>\n",
              "      <td>0.329502</td>\n",
              "      <td>0.340996</td>\n",
              "      <td>0.354406</td>\n",
              "      <td>0.356322</td>\n",
              "      <td>0.360153</td>\n",
              "      <td>0.371648</td>\n",
              "      <td>0.385057</td>\n",
              "      <td>0.394636</td>\n",
              "      <td>0.400383</td>\n",
              "      <td>0.421456</td>\n",
              "      <td>0.434866</td>\n",
              "      <td>0.452107</td>\n",
              "      <td>0.459770</td>\n",
              "      <td>0.471264</td>\n",
              "      <td>0.484674</td>\n",
              "      <td>0.492337</td>\n",
              "      <td>0.494253</td>\n",
              "      <td>0.503831</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.949153</td>\n",
              "      <td>0.815981</td>\n",
              "      <td>0.210654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.244552</td>\n",
              "      <td>0.251816</td>\n",
              "      <td>0.259080</td>\n",
              "      <td>0.261501</td>\n",
              "      <td>0.261501</td>\n",
              "      <td>0.266344</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.300242</td>\n",
              "      <td>0.300242</td>\n",
              "      <td>0.309927</td>\n",
              "      <td>0.324455</td>\n",
              "      <td>0.338983</td>\n",
              "      <td>0.353511</td>\n",
              "      <td>0.346247</td>\n",
              "      <td>0.382567</td>\n",
              "      <td>0.406780</td>\n",
              "      <td>0.423729</td>\n",
              "      <td>0.445521</td>\n",
              "      <td>0.447942</td>\n",
              "      <td>0.474576</td>\n",
              "      <td>0.476998</td>\n",
              "      <td>0.491525</td>\n",
              "      <td>0.486683</td>\n",
              "      <td>0.479419</td>\n",
              "      <td>0.472155</td>\n",
              "      <td>0.445521</td>\n",
              "      <td>0.440678</td>\n",
              "      <td>0.406780</td>\n",
              "      <td>0.394673</td>\n",
              "      <td>0.389830</td>\n",
              "      <td>0.368039</td>\n",
              "      <td>0.363196</td>\n",
              "      <td>0.358354</td>\n",
              "      <td>0.351090</td>\n",
              "      <td>0.336562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 188 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0.9248658418655396  0.9928443431854248  ...  0.0.77  0.0.78\n",
              "0            0.981324            0.964346  ...     0.0     0.0\n",
              "1            0.985486            0.804064  ...     0.0     0.0\n",
              "2            0.827320            0.755155  ...     0.0     2.0\n",
              "3            0.965517            0.620690  ...     0.0     0.0\n",
              "4            0.949153            0.815981  ...     0.0     0.0\n",
              "\n",
              "[5 rows x 188 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBJn-xmSbFQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, df_val = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0sSv4J1aLtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df_train.iloc[:, :-1]\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "x_val = df_val.iloc[:, :-1]\n",
        "x_val = np.expand_dims(x_val, axis=-1)\n",
        "y_val = df_val.iloc[:, -1]\n",
        "\n",
        "y_train = utils.to_categorical(y_train, num_classes=num_classes)\n",
        "y_val = utils.to_categorical(y_val, num_classes=num_classes)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5YZZJWZaLtQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edbeecdb-9f3f-4d17-d848-97390553e834"
      },
      "source": [
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1750, 187, 1), (1750, 5), (438, 187, 1), (438, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8jJMzsE_aNJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a99e64dc-17be-455b-9945-4cb73389cfb5"
      },
      "source": [
        "# 查看 y_val 為 nparray\n",
        "y_val"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hfp2IZWofSm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "8583bed3-14c0-4df8-bb0f-18fba3592986"
      },
      "source": [
        "idx = 400\n",
        "print('label: ', y_val[idx])\n",
        "plt.plot(x_val[idx])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label:  [0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0415abacf8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAXpklEQVR4nO3dfXBc9X3v8fdnd/WELdtgKQT8TDFpHUKDoyHODWnJEIKhjd2QJjU3aZJebnzbgTSZNHTIcIdL6WRakmk67YxJ4jSUJCU4NG0ahTg1bUqgN+HBAj+AH1HAYBsbC2xsUz/Isr79Y89Ku7JkrfHKu2f1ec1ovHv2Z+3XR/JHX/3O75yjiMDMzNIvU+0CzMysMhzoZmZ1woFuZlYnHOhmZnXCgW5mVidy1Xrjtra2mD17drXe3swslZ588slXIqJ9uNeqFuizZ8+mq6urWm9vZpZKkl4Y6TVPuZiZ1QkHuplZnXCgm5nVCQe6mVmdcKCbmdWJUQNd0t2S9kh6ZoTXJelvJXVLWi9pfuXLNDOz0ZTTod8DLDzJ69cAc5OPpcBXT78sMzM7VaMGekQ8Auw9yZDFwLcj7zFgiqTzKlXgUKu37eVL/7qZ/n5f9tfMrFgl5tCnAduLnu9Itp1A0lJJXZK6enp63tCbrdv+Gnf97Je83tv3hv6+mVm9OqMHRSNieUR0RERHe/uwZ66OqrU5f3LrwSMOdDOzYpUI9J3AjKLn05NtY6K1uQGAg0eOjdVbmJmlUiUCvRP4eLLaZQGwPyJ2VeDzDssdupnZ8Ea9OJek+4ArgDZJO4D/BzQARMTXgJXAtUA3cAj4g7EqFtyhm5mNZNRAj4jrR3k9gBsrVtEo3KGbmQ0vdWeKFgL9gAPdzKxE6gJ9kqdczMyGlbpAb8plaMjKUy5mZkOkLtAl0drcwIHD7tDNzIqlLtAhP4/uDt3MrFSKA90duplZsXQGelODO3QzsyHSGeiecjEzO0FKA73BUy5mZkOkMtAntbhDNzMbKpWB3trcwOu9fb7JhZlZkVQG+qTmHBH4JhdmZkVSGei+QJeZ2YlSGui+nouZ2VApDfTkiouH3aGbmRWkNNDdoZuZDZXSQPccupnZUCkPdHfoZmYFqQz0wk0ufNciM7NBqQx03+TCzOxEqQz0wk0uPOViZjYolYEOvuKimdlQqQ30loYsh48dr3YZZmY1I7WBnsvKF+cyMyuS2kDPZjL0OdDNzAakNtBzGXHcgW5mNiC1gZ7NiGPH+6tdhplZzUhtoLtDNzMrldpAz2bkOXQzsyKpDXR36GZmpVIb6F7lYmZWqqxAl7RQ0hZJ3ZJuGeb1mZIekrRG0npJ11a+1FL5Dt0HRc3MCkYNdElZYBlwDTAPuF7SvCHD/i9wf0RcCiwB7qp0oUNls55DNzMrVk6HfhnQHRHPRUQvsAJYPGRMAJOSx5OBlypX4vA8h25mVqqcQJ8GbC96viPZVux24GOSdgArgU8P94kkLZXUJamrp6fnDZQ7KJsRfccd6GZmBZU6KHo9cE9ETAeuBb4j6YTPHRHLI6IjIjra29tP6w3doZuZlSon0HcCM4qeT0+2FbsBuB8gIh4FmoG2ShQ4Eq9yMTMrVU6grwbmSpojqZH8Qc/OIWNeBK4EkPRr5AP99OZURuFVLmZmpUYN9IjoA24CVgGbyK9m2SDpDkmLkmF/AnxK0jrgPuCTETGm7XPOq1zMzErkyhkUESvJH+ws3nZb0eONwLsrW9rJeQ7dzKyUzxQ1M6sTqQ10d+hmZqVSG+jZJNDHeKrezCw1UhvouYwA3KWbmSVSG+jZbD7QPY9uZpaX2kB3h25mViq1gZ7N5Et3h25mlpfaQHeHbmZWKrWBns0U5tB9+r+ZGaQ40N2hm5mVSm2gD3Tovia6mRmQ4kDPZd2hm5kVS22ge5WLmVmp1Aa659DNzEqlNtC9ysXMrFRqA90duplZqdQG+mCH7kA3M4MUB3ouOSjqDt3MLC+1ge516GZmpVIb6F6HbmZWKrWB7lUuZmalUhvoXuViZlYqtYHuVS5mZqVSG+he5WJmViq1ge4O3cysVGoDfXAO3QdFzcwgxYHudehmZqVSG+heh25mViq1ge45dDOzUqkNdK9yMTMrVVagS1ooaYukbkm3jDDmI5I2Stog6buVLfNE7tDNzErlRhsgKQssA64CdgCrJXVGxMaiMXOBLwDvjoh9kt40VgUXeJWLmVmpcjr0y4DuiHguInqBFcDiIWM+BSyLiH0AEbGnsmWeyB26mVmpcgJ9GrC96PmOZFuxi4CLJP1c0mOSFg73iSQtldQlqaunp+eNVZwY6NC9bNHMDKjcQdEcMBe4Arge+IakKUMHRcTyiOiIiI729vbTekN36GZmpcoJ9J3AjKLn05NtxXYAnRFxLCKeB7aSD/gxI4lsRl7lYmaWKCfQVwNzJc2R1AgsATqHjPkX8t05ktrIT8E8V8E6h5XNyB26mVli1ECPiD7gJmAVsAm4PyI2SLpD0qJk2CrgVUkbgYeAmyPi1bEquiCXkVe5mJklRl22CBARK4GVQ7bdVvQ4gM8lH2eMO3Qzs0GpPVMUCh26A93MDFIe6NlMxh26mVki1YGey4i+455DNzODlAe659DNzAalOtBzWc+hm5kVpDrQ3aGbmQ1KdaDnMvK1XMzMEqkOdK9yMTMblOpA95miZmaDUh3onkM3MxuU6kD3maJmZoNSHeju0M3MBqU60L0O3cxsULoD3atczMwGpDzQvcrFzKwg1YGezYg+n1hkZgakPNA9h25mNijVgZ7NZBzoZmaJVAd6zssWzcwGpDrQsz6xyMxsQKoDPd+he5WLmRmkPNDdoZuZDUp1oHsO3cxsUKoDPZvJ+AYXZmaJVAd6LusO3cysINWB7jl0M7NBqQ50r3IxMxuU6kDPZkR/QL+7dDOzdAd6LiMAjocD3cws1YGezeTL9zy6mVnKA73QoXuli5lZmYEuaaGkLZK6Jd1yknEfkhSSOipX4siyhSkXr0U3Mxs90CVlgWXANcA84HpJ84YZ1wp8Bni80kWOJJctdOhe6WJmVk6HfhnQHRHPRUQvsAJYPMy4PwfuBI5UsL6TGujQPeViZlZWoE8Dthc935FsGyBpPjAjIn58sk8kaamkLkldPT09p1zsUJ5DNzMbdNoHRSVlgK8AfzLa2IhYHhEdEdHR3t5+um/tVS5mZkXKCfSdwIyi59OTbQWtwMXAzyRtAxYAnWfiwKg7dDOzQeUE+mpgrqQ5khqBJUBn4cWI2B8RbRExOyJmA48BiyKia0wqLjI4h+6DomZmowZ6RPQBNwGrgE3A/RGxQdIdkhaNdYEn4w7dzGxQrpxBEbESWDlk220jjL3i9MsqT6FD7/M6dDOzlJ8pmvWyRTOzglQHemGVi6dczMxSHug5n1hkZjYg1YE+MIfuVS5mZukOdHfoZmaDUh3oWS9bNDMbkOpAzxVO/feyRTOzlAd6smzx2HHPoZuZpTrQmxuyABzpO17lSszMqi/Vgd6SBPrhXnfoZmapDvTmhnz5R465QzczS3mgJx26A93MLN2B3pTLIMFRB7qZWboDXRLNuaw7dDMzUh7oAC2NWY4c80FRM7PUB3pzLuMO3cyMegj0Rk+5mJlBPQR6LuuDomZm1EGgt7hDNzMD6iDQmxsyPihqZkYdBHpLQ5bDve7QzcxSH+jNDVlfnMvMjHoJdHfoZmbpD/SWhixH+jyHbmaW+kBvbsh4Dt3MjDoI9JZkDj3Ct6Ezs/Et9YHe1JAlAo562sXMxrnUB3rhrkVHvRbdzMa51Ae6b3JhZpaX+kBvacz/ExzoZjbelRXokhZK2iKpW9Itw7z+OUkbJa2X9FNJsypf6vCac/kO3fcVNbPxbtRAl5QFlgHXAPOA6yXNGzJsDdAREZcA3we+VOlCR9Lc6CkXMzMor0O/DOiOiOciohdYASwuHhARD0XEoeTpY8D0ypY5ssJBUXfoZjbelRPo04DtRc93JNtGcgPwk+FekLRUUpekrp6envKrPIlmB7qZGVDhg6KSPgZ0AF8e7vWIWB4RHRHR0d7eXpH3HOzQvWzRzMa3XBljdgIzip5PT7aVkPQ+4FbgNyPiaGXKG11zQ7LKxaf/m9k4V06HvhqYK2mOpEZgCdBZPEDSpcDXgUURsafyZY5soEP3JXTNbJwbNdAjog+4CVgFbALuj4gNku6QtCgZ9mVgIvCPktZK6hzh01VcU+HEInfoZjbOlTPlQkSsBFYO2XZb0eP3Vbiusg2c+u9ruZjZOJf6M0UbsiIjd+hmZqkPdEn5+4p62aKZjXOpD3RIbkPnQDezca5uAt0dupmNd3UR6C2NWV8P3czGvboI9OaGjDt0Mxv36iLQWzyHbmZWH4HuOXQzszoKdF+cy8zGuzoKdHfoZja+1UWgtzRkHOhmNu7VRaB7Dt3MrE4C/eyzGjlw+BiHevuqXYqZWdXURaC/9fxJ9Ads2nWg2qWMaMe+Qzzx/F4iotqlmFmdqotAf9v0yQA8vWN/lSsZ3v5Dx1iy/DE+8vVHuf4bj/HUi/uqXZKZ1aG6CPQ3T2qmbWIjz7xUex16RHDz99exe/8Rbnzvr9C953Wuu+sXfPq+NRzvd7duZpVTF4EuiYunTeaZnbXVoR/uPc5nv7eWBze+zC3X/Co3X/2rPHzze/noO2fyo3Uv8eyeg9Uu0czqSF0EOsDF50/m2T2v18zyxYjgE3c/Qee6l7j56rdww+VzAJjQlGPRr58PwCsHe6tZopnVmfoJ9GmTOd4fbKyRA6Obdh3kiW17ufXaX+PG916IpIHX2lqbAOh5/Ui1yjOzOlQ3gV44MFor0y4PrH+JbEZcN3/6Ca+1J4HuDt3MKqluAv38yc1MndDI+hpY6RIR/Gj9S7z7wjbOmdB4wuutTTkacxl6Xj9aherMrF7VTaBL4u0zprCmBpYErtuxn+17D/OBS84b9nVJtE9s4pWDDnQzq5xctQuopPmzzuanm/fw2qFeppx1Ymc81g719vGn31/PQ5v30JjL8P63vnnEsW2tTe7Qzayi6qZDB7h0xhQA1mx/rSrvv2rDbh5Yv4tr33Ye931qAZNbGkYc2z6xiR536GZWQXUV6L8+YwoZwZoXqxToz7zMmyc1c+eHLuEds84+6dj21kZeed0HRc2scuoq0Cc05XjLmydVZR79yLHjPLy1h6vmnUsmo1HHt09sYu9/HfXZomZWMXUV6ACXzpzC2hdfo/8MB+V/PvsKh48d5/1vPbes8W2tTfQH7P0vd+lmVhl1F+jzZ57NwaN9/P/uV87I++05cIS7ftbN8kd+SWtzjgUXTC3r77VPTE4u8jy6mVVI3QX6VfPO5YL2Cdx471MDJxmt2rCbS25fxY59hyr+fn/971v50r9uYfW2fVx36TQasuXt0sLZoq94pYuZVUjdBfrklgb+4YZ3MqmlgRu+tZr9h4/x1/+2lQNH+rj38Rc51NvHrT94ms27T/8SAYd7j/PAul188NJpPPvFa/izxReX/XfdoZtZpdXVOvSC86e08NWPzed3lv2c3//m42zefZC2iY2seOJFDh3NB/vm3Qe5//+8i5u/v46jff38/oJZ/HDtTnoOHuWuj76DxlyGiBi4BsvDW3u4+PxJTE2CGODBjbs5eLSPD3dML7szL3CHbmaVVlagS1oI/A2QBf4uIv5yyOtNwLeBdwCvAr8XEdsqW+qpuWT6FD7+rtnc84ttTJvSwhc/eDGf/PvVfOvRF5g99SyefGEff3DPah7Z2kNjLsOP1+8ilxF9/cE3/vM5Zk09iz/70Ub+8rq3caj3OJ++bw3vnHMOK5YuoPd4P7v3H2HFE9uZfnYLC+aUN29ebEJjlpaGrDt0M6uYUQNdUhZYBlwF7ABWS+qMiI1Fw24A9kXEhZKWAHcCvzcWBZ+Kz1/9FjbvPsDHFsziNy9q58I3TeS1Q7380x/9Dz789Ud5ZGsPi99+Prf99jwe3Pgyl1/Yxl/8ZBN/89Nn6e8PJPije5+iISPaJjbx+PN7+fKqLTywfhcv7s3Px3/myrllLVMcShJtrY3u0M2sYjTaPS4lvQu4PSKuTp5/ASAi/qJozKpkzKOScsBuoD1O8sk7Ojqiq6urAv+E8r346iH6+vu5oH0iTzy/l+8+/gJf/ODbmNA0+HNtz4EjvO8rDzNr6gSW/c/5/O9vr6bn4FEe+OP38NkVa1i9bR/nTmriM1dexISmLFfNO5ezGt/YzNV1d/2cLbsPcv6Ulkr9E80sBf74yrl8ILkvwqmS9GREdAz3WjlJNA3YXvR8B/DOkcZERJ+k/cBUoGTtoKSlwFKAmTNnllV8Jc2cetbA48vmnMNlc845YcybJjXzH5+/gtbmHE25LD+88XIO9fYxdWITf/Xht3PPL7bxh1dcwJtam0+7nhsuv4AfP/3SaX8eM0uXk10W5HSc0YOiEbEcWA75Dv1MvvepaCs68NnSmKWlMQvkfyDc9oF5FXuf37rkPH5rhCsympmdqnKWZuwEZhQ9n55sG3ZMMuUymfzBUTMzO0PKCfTVwFxJcyQ1AkuAziFjOoFPJI9/F/iPk82fm5lZ5Y065ZLMid8ErCK/bPHuiNgg6Q6gKyI6gW8C35HUDewlH/pmZnYGlTWHHhErgZVDtt1W9PgI8OHKlmZmZqei7k79NzMbrxzoZmZ1woFuZlYnHOhmZnVi1FP/x+yNpR7ghTf419sYchZqDXKNleEaK8M1VkYt1DgrItqHe6FqgX46JHWNdC2DWuEaK8M1VoZrrIxar9FTLmZmdcKBbmZWJ9Ia6MurXUAZXGNluMbKcI2VUdM1pnIO3czMTpTWDt3MzIZwoJuZ1YnUBbqkhZK2SOqWdEu16wGQNEPSQ5I2Stog6TPJ9tsl7ZS0Nvm4tsp1bpP0dFJLV7LtHEn/JunZ5M+zq1TbW4r201pJByR9thb2oaS7Je2R9EzRtmH3m/L+Nvn+XC9pfpXq+7KkzUkNP5A0Jdk+W9Lhov35tbGu7yQ1jvi1lfSFZB9ukXR1FWv8XlF92yStTbZXZT+OKiJS80H+8r2/BC4AGoF1wLwaqOs8YH7yuBXYCswDbgc+X+36iurcBrQN2fYl4Jbk8S3AnTVQZ5b8fWln1cI+BH4DmA88M9p+A64FfgIIWAA8XqX63g/kksd3FtU3u3hclffhsF/b5P/OOqAJmJP8n89Wo8Yhr/8VcFs19+NoH2nr0C8DuiPiuYjoBVYAi6tcExGxKyKeSh4fBDaRv89qGiwGvpU8/hbwO1WspeBK4JcR8UbPJK6oiHiE/HX+i4203xYD3468x4Apksb0PoPD1RcRD0ZEX/L0MfJ3GquaEfbhSBYDKyLiaEQ8D3ST/78/pk5WoyQBHwHuG+s6TkfaAn24G1bXVHBKmg1cCjyebLop+bX37mpNZxQJ4EFJTyY37AY4NyJ2JY93A+dWp7QSSyj9j1NL+7BgpP1Wi9+j/4v8bw0FcyStkfSwpPdUq6jEcF/bWtyH7wFejohni7bV0n4E0hfoNU3SROCfgM9GxAHgq8CvAG8HdpH/la2aLo+I+cA1wI2SfqP4xcj/LlnVdazK3+ZwEfCPyaZa24cnqIX9NhJJtwJ9wL3Jpl3AzIi4FPgc8F1Jk6pUXs1/bYtcT2mTUUv7cUDaAr2cG1ZXhaQG8mF+b0T8M0BEvBwRxyOiH/gGZ+DXxpOJiJ3Jn3uAHyT1vFyYEkj+3FO9CoH8D5unIuJlqL19WGSk/VYz36OSPgn8NvDR5IcOyTTGq8njJ8nPT19UjfpO8rWtmX0IAze+vw74XmFbLe3HYmkL9HJuWH3GJfNr3wQ2RcRXirYXz51+EHhm6N89UyRNkNRaeEz+oNkzlN7g+xPAD6tT4YCSTqiW9uEQI+23TuDjyWqXBcD+oqmZM0bSQuBPgUURcahoe7ukbPL4AmAu8NyZri95/5G+tp3AEklNkuaQr/GJM11fkfcBmyNiR2FDLe3HEtU+KnuqH+RXEWwl/xPx1mrXk9R0OflfudcDa5OPa4HvAE8n2zuB86pY4wXkVw6sAzYU9h0wFfgp8Czw78A5VaxxAvAqMLloW9X3IfkfMLuAY+Tnc28Yab+RX92yLPn+fBroqFJ93eTnoQvfj19Lxn4o+fqvBZ4CPlDFfTji1xa4NdmHW4BrqlVjsv0e4A+HjK3Kfhztw6f+m5nVibRNuZiZ2Qgc6GZmdcKBbmZWJxzoZmZ1woFuZlYnHOhmZnXCgW5mVif+G2kWC82tZd9OAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AlqHd5bow0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2c3f6ab1-a410-4f71-dccf-1d639229826d"
      },
      "source": [
        "# 統計一下第 0個類別有 1468 筆資料, 依此類推\n",
        "# 類別1 是正常情況, 有 1468筆, 類別2 是不正常的心電圖有 51筆, 各個類別的數量不平衡\n",
        "# 這個模型自然而然會傾向於類別1的方向預測，解決方式是 把類別1 抽樣成跟其他數量差不多\n",
        "# \n",
        "np.unique(np.argmax(y_train, axis=-1), return_counts=True), np.unique(np.argmax(y_val, axis=-1), return_counts=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([0, 1, 2, 3, 4]), array([1452,   52,  117,   16,  113])),\n",
              " (array([0, 1, 2, 3, 4]), array([369,  10,  18,   3,  38])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19sTD1o8_9zO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f8d7acb-16ae-4ea9-b55f-1d9899d8ca8b"
      },
      "source": [
        "# 計算個類別比重程度\n",
        "per = []\n",
        "for i in np.unique(np.argmax(y_train, axis=-1), return_counts=True)[1]:\n",
        "  per.append( 1 // (i / np.sum(np.unique(np.argmax(y_train, axis=-1), return_counts=True)[1])))\n",
        "per"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 33.0, 14.0, 109.0, 15.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Git1WfBWaLtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN 用 conv1D，心電圖訊號是1維訊號，X軸是時間，Y軸是電壓大小\n",
        "# 轉成數值的方式去看，每一個時間點有電壓大小的數值\n",
        "def build_model():\n",
        "    inputs = layers.Input(shape=x_train.shape[1:])\n",
        "    conv1_1 = layers.Conv1D(64, (6), activation='relu')(inputs)\n",
        "    conv1_1 = layers.BatchNormalization()(conv1_1)\n",
        "    pool1 = layers.Conv1D(64, 3, strides=(2), padding=\"same\")(conv1_1)\n",
        "    conv2_1 = layers.Conv1D(128, (3), activation='relu')(pool1)\n",
        "    conv2_1 = layers.BatchNormalization()(conv2_1)\n",
        "    pool2 = layers.Conv1D(128, 3, strides=(2), padding=\"same\")(conv2_1)\n",
        "    conv3_1 = layers.Conv1D(256, (3), activation='relu')(pool2)\n",
        "    conv3_1 = layers.BatchNormalization()(conv3_1)\n",
        "    pool3 = layers.Conv1D(256, 3, strides=(2), padding=\"same\")(conv3_1)\n",
        "    flatten = layers.Flatten()(pool3)\n",
        "    dense_end1 = layers.Dense(64, activation='relu')(flatten)\n",
        "    dense_end2 = layers.Dense(32, activation='relu')(dense_end1)\n",
        "    main_output = layers.Dense(num_classes, activation='softmax', name='main_output')(dense_end2)\n",
        "    \n",
        "    model = models.Model(inputs, main_output)\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFNobbp5aLtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "f92eb433-d0db-4f73-e445-2f65b93827fb"
      },
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 187, 1)]          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 182, 64)           448       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 182, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 91, 64)            12352     \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 89, 128)           24704     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 89, 128)           512       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 45, 128)           49280     \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 43, 256)           98560     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 43, 256)           1024      \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 22, 256)           196864    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5632)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                360512    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "main_output (Dense)          (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 746,757\n",
            "Trainable params: 745,861\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZNJNwYJaLtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
        "            tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIHFGNEXaLtd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5fc087d3-6300-482c-8758-5e85bfa20282"
      },
      "source": [
        "# class_weight 可以讓每一個類別的比重吃多少，類別 2~4 的資料量太少，調整比率高一點\n",
        "# 類別1 的資料量相較來講較多，所以調降資料比重\n",
        "history = model.fit(x_train, \n",
        "                    y_train, \n",
        "                    epochs=1000,\n",
        "                    callbacks=callback, \n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    # Weighting Loss function, 如上說明\n",
        "                    class_weight={\n",
        "                        0: per[0],\n",
        "                        1: per[1],\n",
        "                        2: per[2],\n",
        "                        3: per[3],\n",
        "                        4: per[4],\n",
        "                    }\n",
        "                    )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "55/55 [==============================] - 1s 20ms/step - loss: 6.8167 - accuracy: 0.4731 - val_loss: 1.1888 - val_accuracy: 0.7968\n",
            "Epoch 2/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 4.5067 - accuracy: 0.5514 - val_loss: 2.1308 - val_accuracy: 0.0228\n",
            "Epoch 3/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 4.4977 - accuracy: 0.5766 - val_loss: 3.7093 - val_accuracy: 0.0228\n",
            "Epoch 4/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 3.5920 - accuracy: 0.6463 - val_loss: 6.1087 - val_accuracy: 0.0228\n",
            "Epoch 5/1000\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 3.4585 - accuracy: 0.6943 - val_loss: 1.3803 - val_accuracy: 0.7009\n",
            "Epoch 6/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 3.1317 - accuracy: 0.7011 - val_loss: 1.6108 - val_accuracy: 0.3516\n",
            "Epoch 7/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 2.2751 - accuracy: 0.7051 - val_loss: 3.8582 - val_accuracy: 0.0434\n",
            "Epoch 8/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 1.9500 - accuracy: 0.8154 - val_loss: 2.0788 - val_accuracy: 0.1986\n",
            "Epoch 9/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 2.1561 - accuracy: 0.8154 - val_loss: 1.3106 - val_accuracy: 0.6279\n",
            "Epoch 10/1000\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 1.9627 - accuracy: 0.7720 - val_loss: 1.4998 - val_accuracy: 0.5411\n",
            "Epoch 11/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 1.9519 - accuracy: 0.7874 - val_loss: 0.8004 - val_accuracy: 0.7420\n",
            "Epoch 12/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 1.3831 - accuracy: 0.8480 - val_loss: 1.8300 - val_accuracy: 0.5708\n",
            "Epoch 13/1000\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 2.2502 - accuracy: 0.7337 - val_loss: 1.2096 - val_accuracy: 0.6804\n",
            "Epoch 14/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 1.1155 - accuracy: 0.8366 - val_loss: 1.0063 - val_accuracy: 0.8333\n",
            "Epoch 15/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 1.6406 - accuracy: 0.8057 - val_loss: 1.1701 - val_accuracy: 0.7717\n",
            "Epoch 16/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 5.3727 - accuracy: 0.7640 - val_loss: 7.3895 - val_accuracy: 0.3995\n",
            "Epoch 17/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 8.8402 - accuracy: 0.6097 - val_loss: 3.3209 - val_accuracy: 0.6210\n",
            "Epoch 18/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 4.7297 - accuracy: 0.5617 - val_loss: 2.2588 - val_accuracy: 0.5799\n",
            "Epoch 19/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 3.2397 - accuracy: 0.6663 - val_loss: 1.2870 - val_accuracy: 0.7466\n",
            "Epoch 20/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 2.1696 - accuracy: 0.7223 - val_loss: 0.9819 - val_accuracy: 0.7580\n",
            "Epoch 21/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.8197 - accuracy: 0.8371 - val_loss: 0.6058 - val_accuracy: 0.8288\n",
            "Epoch 22/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.5544 - accuracy: 0.8960 - val_loss: 0.4330 - val_accuracy: 0.8676\n",
            "Epoch 23/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.4222 - accuracy: 0.9189 - val_loss: 0.4689 - val_accuracy: 0.8881\n",
            "Epoch 24/1000\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.3450 - accuracy: 0.9337 - val_loss: 0.5610 - val_accuracy: 0.8105\n",
            "Epoch 25/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.2995 - accuracy: 0.9309 - val_loss: 0.4851 - val_accuracy: 0.8562\n",
            "Epoch 26/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.2095 - accuracy: 0.9503 - val_loss: 0.3815 - val_accuracy: 0.8995\n",
            "Epoch 27/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1697 - accuracy: 0.9674 - val_loss: 0.3853 - val_accuracy: 0.8973\n",
            "Epoch 28/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1368 - accuracy: 0.9737 - val_loss: 0.3207 - val_accuracy: 0.9178\n",
            "Epoch 29/1000\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1196 - accuracy: 0.9777 - val_loss: 0.3387 - val_accuracy: 0.9087\n",
            "Epoch 30/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1265 - accuracy: 0.9760 - val_loss: 0.3845 - val_accuracy: 0.9018\n",
            "Epoch 31/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1594 - accuracy: 0.9749 - val_loss: 0.4662 - val_accuracy: 0.8744\n",
            "Epoch 32/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1113 - accuracy: 0.9783 - val_loss: 0.3401 - val_accuracy: 0.9178\n",
            "Epoch 33/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.0733 - accuracy: 0.9869 - val_loss: 0.3379 - val_accuracy: 0.9224\n",
            "Epoch 34/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.0588 - accuracy: 0.9903 - val_loss: 0.3322 - val_accuracy: 0.9292\n",
            "Epoch 35/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0617 - accuracy: 0.9891 - val_loss: 0.3453 - val_accuracy: 0.9224\n",
            "Epoch 36/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0550 - accuracy: 0.9914 - val_loss: 0.3400 - val_accuracy: 0.9292\n",
            "Epoch 37/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.0506 - accuracy: 0.9920 - val_loss: 0.3801 - val_accuracy: 0.9178\n",
            "Epoch 38/1000\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.0456 - accuracy: 0.9920 - val_loss: 0.3459 - val_accuracy: 0.9269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U8AISSQcaU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cls_report(path):\n",
        "  model = tf.keras.models.load_model(path)\n",
        "  pred = np.argmax(model.predict(x_val), axis=-1)\n",
        "  print(classification_report(np.argmax(y_val, axis=1), pred))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMPtfDCU54Nj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "284eedf5-39c2-47fb-b5fc-7111f3eb4b4c"
      },
      "source": [
        "cls_report(\"best_model.h5\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95       369\n",
            "           1       0.38      0.50      0.43        10\n",
            "           2       0.68      0.83      0.75        18\n",
            "           3       0.20      0.33      0.25         3\n",
            "           4       1.00      0.79      0.88        38\n",
            "\n",
            "    accuracy                           0.92       438\n",
            "   macro avg       0.64      0.68      0.65       438\n",
            "weighted avg       0.93      0.92      0.92       438\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0cAilz0_Jxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy = 0.92"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}